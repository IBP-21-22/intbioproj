{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wheel\n",
    "# !pip install h5py\n",
    "# !pip install torch==1.10.0+cu113 torchvision==0.11.1+cu113 torchaudio==0.10.0+cu113 -f https://download.pytorch.org/whl/cu113/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import h5py\n",
    "import yaml\n",
    "import pickle\n",
    "import os\n",
    "import argparse\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass, field\n",
    "import contextlib\n",
    "import math\n",
    "from heapq import merge\n",
    "from math import floor\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import product\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from scipy.stats import spearmanr, pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_YAM = False\n",
    "if not USE_YAM:\n",
    "    Y_ID = ''\n",
    "    data = pd.read_csv('../processed_data/utrs.csv')\n",
    "    data = data[['Gene Name', 'foreign']].dropna(axis=0).rename(columns={'Gene Name': 'gene', 'foreign':'seq'})\n",
    "else:\n",
    "    Y_ID = 'YAM'\n",
    "    data = pd.read_csv('../processed_data/yamanishi_data.csv', header = 0)\n",
    "    data = data.loc[data.utr_seq.notna()]\n",
    "    data = data.loc[data.gene.notna()]\n",
    "    data.drop_duplicates(inplace = True)\n",
    "    data = data[['gene', 'utr_seq']].dropna(axis=0).rename(columns={'utr_seq':'seq'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene</th>\n",
       "      <th>seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VPS8</td>\n",
       "      <td>ACATTTCTAAATATTTAATACAACTTTGGTTACATAAAAGTAAAAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SSA1</td>\n",
       "      <td>AGCCAATTGGTGCGGCAATTGATAATAACGAAAATGTCTTTTAATG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ERP2</td>\n",
       "      <td>AGAACTTTTCAATCTACGAAAAATATATGTCCGCAATATAGAACAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FUN14</td>\n",
       "      <td>AGCAAGACAAATGACCAGATATAAACGAGGGTTATATTCTTTCGTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SPO7</td>\n",
       "      <td>AAAGAGTTGGAGGGCTTCTTCCTTCGAATAAGAGGTCATATTTACC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    gene                                                seq\n",
       "4   VPS8  ACATTTCTAAATATTTAATACAACTTTGGTTACATAAAAGTAAAAT...\n",
       "5   SSA1  AGCCAATTGGTGCGGCAATTGATAATAACGAAAATGTCTTTTAATG...\n",
       "6   ERP2  AGAACTTTTCAATCTACGAAAAATATATGTCCGCAATATAGAACAC...\n",
       "7  FUN14  AGCAAGACAAATGACCAGATATAAACGAGGGTTATATTCTTTCGTT...\n",
       "8   SPO7  AAAGAGTTGGAGGGCTTCTTCCTTCGAATAAGAGGTCATATTTACC..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KMER_SIZE = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficiency element\n",
    "eff_el1 = \"TATATA\"\n",
    "eff_el2 = \"TTTTTATA\"\n",
    "eff_ctrl = \"GCGCGC\"\n",
    "# Mutational scan of efficiency element?\n",
    "# Positioning element\n",
    "pos_el = \"AAWAAA\"\n",
    "# Puf protein binding sites\n",
    "puf1_2 = \"TAATNNNTAAT\"\n",
    "puf3 = \"TGTANATA\"\n",
    "puf4 = \"TGTANANTA\"\n",
    "puf5 = \"TGTANNNNTA\"\n",
    "puf6 = \"TTGT\"\n",
    "# Poly-T sequences\n",
    "poly_t = \"TTTTTTTT\"\n",
    "_elems = [eff_el1, eff_el2, eff_ctrl,\n",
    "           pos_el,\n",
    "           puf1_2, puf3, puf4, puf5, puf6,\n",
    "           poly_t]\n",
    "specElements = []\n",
    "# expand elements above by replacing Ns with A,T,G or C and Ws with A or T\n",
    "for elem in _elems:\n",
    "    specElements.extend([''.join(y) for y in list(product(*(['A', 'T', 'G', 'C'] if x=='N'  else (\n",
    "        ['A', 'T'] if x=='W' else  (x,)) for x in elem)))])\n",
    "# augment elements using all contiguous subsequences of size k - 1  of them, if the element size is larger than K\n",
    "specElements = list(set(specElements + sum([[x[:-1], x[1:]] for x in specElements if len(x) > KMER_SIZE], [])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.seq.apply(lambda x: len(x)>=KMER_SIZE)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(specElements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Stride (overlap) of subsequence reading based on current subsequence entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(string):\n",
    "    \"Calculates the Shannon entropy of a string\"\n",
    "\n",
    "    # get probability of chars in string\n",
    "    prob = [ float(string.count(c)) / len(string) for c in dict.fromkeys(list(string)) ]\n",
    "\n",
    "    # calculate the entropy\n",
    "    entropy = - sum([ p * math.log(p) / math.log(2.0) for p in prob ])\n",
    "\n",
    "    return min(1, abs(entropy) / 2)\n",
    "def compute_stride(seq):\n",
    "    return max(1, round(len(seq) * (1 - entropy(seq))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply stride computation to the 20 first specific elements above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GTATCACTA', 1),\n",
       " ('TGTATGGCTA', 1),\n",
       " ('GTAGCATTA', 1),\n",
       " ('TGTAAAT', 2),\n",
       " ('GTACAGTA', 1),\n",
       " ('TGTACGGTTA', 1),\n",
       " ('GTATAATTA', 3),\n",
       " ('GTACGATTA', 1),\n",
       " ('GCGCGC', 3),\n",
       " ('GTATTCATA', 1),\n",
       " ('AATTGTTAAT', 3),\n",
       " ('TGTAGGTCTA', 1),\n",
       " ('AATTCTTAAT', 3),\n",
       " ('TGTAGACTA', 1),\n",
       " ('TGTAACTATA', 1),\n",
       " ('TGTATTAATA', 3),\n",
       " ('TGTAGAACTA', 1),\n",
       " ('TGTAAACTA', 1),\n",
       " ('AATTTATAAT', 5),\n",
       " ('TGTATTTTT', 5)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_strides = [compute_stride(s) for s in specElements[:20]]\n",
    "list(map(tuple, zip(*[specElements,search_strides])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_kmer_adaptive(seq, sub, stride):\n",
    "    # Search positions of subsequence in seq, while respecting provided stride, in case of a hit\n",
    "    found = []\n",
    "    pos = 0\n",
    "    while pos < len(seq):\n",
    "        if seq[pos:pos+len(sub)] == sub:\n",
    "            found.append(pos)\n",
    "            pos += stride\n",
    "        else:\n",
    "            pos += 1\n",
    "    return found\n",
    "\n",
    "def get_kmers_adaptive(seq, k, min_stride):\n",
    "    #Get K-Mers existing in the sequence seq, while computing the stride based on entropy and \n",
    "    #respecting a minimum stride\n",
    "    kmers = []\n",
    "    pos = 0\n",
    "    while pos < len(seq):\n",
    "        if pos + k > len(seq):\n",
    "            break\n",
    "        kmers.append(seq[pos:pos+k])\n",
    "        pos += max(min_stride, compute_stride(kmers[-1]))\n",
    "    return kmers\n",
    "\n",
    "def search_all_kmers(seq, specElements, kmer_size, min_stride=1):\n",
    "    # sort specific elements based on their length, so in case of overlap hit,\n",
    "    # found sequences are ordered in a logical matter\n",
    "    # eg AGT and AGTAC found both in position j, the produced string will be \"AGT AGTAC\"\n",
    "    specElements = sorted(specElements, key=len)\n",
    "    pairs = []\n",
    "    # Retrieve the found positions of each element and merge all the found elements positions together\n",
    "    for elem in specElements:\n",
    "        pos = search_kmer_adaptive(seq, elem, compute_stride(elem))\n",
    "        new_words_len = [len(elem) for _ in range(len(pos))]\n",
    "        new_words = [elem for _ in range(len(pos))]\n",
    "        new_pairs = list(map(tuple, zip(*[pos,new_words_len,new_words]))) \n",
    "        pairs = merge(pairs, new_pairs)\n",
    "    pairs = [(x[0],x[0] + x[1], x[2]) for x in list(pairs)]\n",
    "    # For the remaining intervals, in between found elements, get the kmers of the specific kmer_size\n",
    "    # and with the minimum stride min_stride\n",
    "    final_sequence = []\n",
    "    # start of sequence (before findings)\n",
    "    if not pairs:\n",
    "        return get_kmers_adaptive(seq, kmer_size, min_stride)\n",
    "    final_sequence =  get_kmers_adaptive(seq[:pairs[0][0] + compute_stride(pairs[0][2])], kmer_size, min_stride)\n",
    "    # middle of sequence (with intertwined findings)\n",
    "    for cnt in range(len(pairs) - 1):\n",
    "        final_sequence.append(pairs[cnt][2])\n",
    "        final_sequence.extend(\n",
    "            get_kmers_adaptive(\n",
    "                seq[pairs[cnt][1] - compute_stride(pairs[cnt][2]) :\n",
    "                    pairs[cnt+1][0] + compute_stride(pairs[cnt + 1][2])], kmer_size, min_stride))\n",
    "    # end of sequence (after findings)\n",
    "    final_sequence.append(pairs[-1][2])\n",
    "    final_sequence.extend(\n",
    "        get_kmers_adaptive(seq[pairs[-1][1] - compute_stride(pairs[-1][2]):],\n",
    "                           kmer_size, min_stride))\n",
    "    return final_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of how the first 3'UTR sequence is split based on the algorithm above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ACATTTCTAAATATTTAATACAACTTTGGTTACATAAAAGTAAAATTTATACACCTCATTTCATTATGTAGATTCATATATAGAATACCAATTATGATTG'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.seq.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACATTT',\n",
       " 'ATTTCT',\n",
       " 'TTCTAA',\n",
       " 'CTAAAT',\n",
       " 'AAATAT',\n",
       " 'AATATTTAAT',\n",
       " 'TTAATA',\n",
       " 'ATACAA',\n",
       " 'ACAACT',\n",
       " 'AACTTT',\n",
       " 'CTTTGG',\n",
       " 'TTGGTT',\n",
       " 'GTTACA',\n",
       " 'TTACAT',\n",
       " 'ACATAA',\n",
       " 'ATAAAA',\n",
       " 'AAGTAA',\n",
       " 'GTAAAA',\n",
       " 'AAAATT',\n",
       " 'ATTTAT',\n",
       " 'TATACA',\n",
       " 'TACACC',\n",
       " 'CACCTC',\n",
       " 'CCTCAT',\n",
       " 'TCATTT',\n",
       " 'ATTTCA',\n",
       " 'TTCATT',\n",
       " 'CATTAT',\n",
       " 'TGTAGAT',\n",
       " 'TGTAGATT',\n",
       " 'TTCATA',\n",
       " 'TATATA',\n",
       " 'ATAGAA',\n",
       " 'AGAATA',\n",
       " 'AATACC',\n",
       " 'TACCAA',\n",
       " 'CCAATT',\n",
       " 'CAATTA',\n",
       " 'ATTATG',\n",
       " 'TATGAT',\n",
       " 'TGATTG']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_all_kmers(data.seq.iloc[0], specElements, KMER_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1888e898cb904664b3fe4d64197125cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4869 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "preprocessed_seq = data.seq.progress_apply(search_all_kmers, specElements=specElements, kmer_size=KMER_SIZE)\n",
    "corpus = [y  for x in preprocessed_seq for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "head: cannot open '../processed_data/utrs_corpus_6' for reading: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "GCORPUS = f'../processed_data/.utrs_corpus_{KMER_SIZE}{Y_ID}'\n",
    "with open(GCORPUS, 'w') as out:\n",
    "    out.write('\\n'.join([' '.join(c) for c in preprocessed_seq]))\n",
    "!head ../processed_data/utrs_corpus_$KMER_SIZE$Y_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBUILD_DIR = 'glove/build'\n",
    "VERBOSE = 2\n",
    "VOCAB_FILE = f'../processed_data/.utrs_vocab_{KMER_SIZE}{Y_ID}'\n",
    "MEMORY = 4\n",
    "WINDOW_SIZE = 10\n",
    "COOCCURRENCE_FILE = f'../processed_data/.utrs_coocc_{KMER_SIZE}_{WINDOW_SIZE}{Y_ID}.bin'\n",
    "COOCCURRENCE_SHUF_FILE = f'../processed_data/.utrs_coocc_{KMER_SIZE}_{WINDOW_SIZE}{Y_ID}.shuf.bin'\n",
    "SEED = 42\n",
    "VECTOR_SIZE = 50\n",
    "THREADS = 8\n",
    "ETA = 0.05\n",
    "X_MAX = 100\n",
    "MAX_ITER = 50\n",
    "SAVE_FILE = f'../processed_data/utrs_embeddings_{KMER_SIZE}_{WINDOW_SIZE}_{VECTOR_SIZE}{Y_ID}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING VOCABULARY\r\n",
      "Processed 0 tokens.\u001b[11G100000 tokens.\u001b[11G200000 tokens.\u001b[11G300000 tokens.\u001b[11G400000 tokens.\u001b[11G500000 tokens.\u001b[11G600000 tokens.\u001b[11G700000 tokens.\u001b[0GProcessed 701064 tokens.\r\n",
      "Counted 5001 unique words.\r\n",
      "Using vocabulary of size 5001.\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!$GBUILD_DIR/vocab_count -verbose $VERBOSE < $GCORPUS > $VOCAB_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COUNTING COOCCURRENCES\n",
      "window size: 10\n",
      "context: symmetric\n",
      "max product: 13752509\n",
      "overflow length: 38028356\n",
      "Reading vocab from file \"../processed_data/.utrs_vocab_6\"...loaded 5001 words.\n",
      "Building lookup table...table contains 21974987 elements.\n",
      "Processing token: 0\u001b[19G100000\u001b[19G200000\u001b[19G300000\u001b[19G400000\u001b[19G500000\u001b[19G600000\u001b[19G700000\u001b[0GProcessed 701064 tokens.\n",
      "Writing cooccurrences to disk.......2 files in total.\n",
      "Merging cooccurrence files: processed 0 lines.\u001b[39G100000 lines.\u001b[39G200000 lines.\u001b[39G300000 lines.\u001b[39G400000 lines.\u001b[39G500000 lines.\u001b[39G600000 lines.\u001b[39G700000 lines.\u001b[39G800000 lines.\u001b[39G900000 lines.\u001b[39G1000000 lines.\u001b[39G1100000 lines.\u001b[39G1200000 lines.\u001b[39G1300000 lines.\u001b[39G1400000 lines.\u001b[39G1500000 lines.\u001b[39G1600000 lines.\u001b[39G1700000 lines.\u001b[39G1800000 lines.\u001b[39G1900000 lines.\u001b[39G2000000 lines.\u001b[39G2100000 lines.\u001b[39G2200000 lines.\u001b[39G2300000 lines.\u001b[39G2400000 lines.\u001b[39G2500000 lines.\u001b[39G2600000 lines.\u001b[39G2700000 lines.\u001b[39G2800000 lines.\u001b[39G2900000 lines.\u001b[39G3000000 lines.\u001b[39G3100000 lines.\u001b[39G3200000 lines.\u001b[39G3300000 lines.\u001b[39G3400000 lines.\u001b[39G3500000 lines.\u001b[39G3600000 lines.\u001b[39G3700000 lines.\u001b[39G3800000 lines.\u001b[39G3900000 lines.\u001b[39G4000000 lines.\u001b[39G4100000 lines.\u001b[39G4200000 lines.\u001b[39G4300000 lines.\u001b[39G4400000 lines.\u001b[39G4500000 lines.\u001b[39G4600000 lines.\u001b[39G4700000 lines.\u001b[39G4800000 lines.\u001b[39G4900000 lines.\u001b[39G5000000 lines.\u001b[39G5100000 lines.\u001b[39G5200000 lines.\u001b[39G5300000 lines.\u001b[39G5400000 lines.\u001b[39G5500000 lines.\u001b[39G5600000 lines.\u001b[39G5700000 lines.\u001b[39G5800000 lines.\u001b[39G5900000 lines.\u001b[39G6000000 lines.\u001b[39G6100000 lines.\u001b[39G6200000 lines.\u001b[0GMerging cooccurrence files: processed 6215849 lines.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.system(f\"{GBUILD_DIR}/cooccur -memory {MEMORY} -vocab-file {VOCAB_FILE} -verbose {VERBOSE} -window-size {WINDOW_SIZE} < {GCORPUS} > {COOCCURRENCE_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using random seed 42\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Shuffling by chunks: processed 0 lines.\u001b[22Gprocessed 6215849 lines.\n",
      "Wrote 1 temporary file(s).\n",
      "Merging temp files: processed 0 lines.\u001b[31G6215849 lines.\u001b[0GMerging temp files: processed 6215849 lines.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"{GBUILD_DIR}/shuffle -memory {MEMORY} -verbose {VERBOSE} -seed {SEED} < {COOCCURRENCE_FILE} > {COOCCURRENCE_SHUF_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TRAINING MODEL\n",
      "Read 6215849 lines.\n",
      "Initializing parameters...Using random seed 1639249056\n",
      "done.\n",
      "vector size: 50\n",
      "vocab size: 5001\n",
      "x_max: 100.000000\n",
      "alpha: 0.750000\n",
      "12/11/21 - 07:57.37PM, iter: 001, cost: 0.030247\n",
      "12/11/21 - 07:57.38PM, iter: 002, cost: 0.026926\n",
      "12/11/21 - 07:57.40PM, iter: 003, cost: 0.026466\n",
      "12/11/21 - 07:57.41PM, iter: 004, cost: 0.025758\n",
      "12/11/21 - 07:57.42PM, iter: 005, cost: 0.024192\n",
      "12/11/21 - 07:57.44PM, iter: 006, cost: 0.021664\n",
      "12/11/21 - 07:57.45PM, iter: 007, cost: 0.018403\n",
      "12/11/21 - 07:57.46PM, iter: 008, cost: 0.015189\n",
      "12/11/21 - 07:57.47PM, iter: 009, cost: 0.012612\n",
      "12/11/21 - 07:57.49PM, iter: 010, cost: 0.010706\n",
      "12/11/21 - 07:57.50PM, iter: 011, cost: 0.009339\n",
      "12/11/21 - 07:57.51PM, iter: 012, cost: 0.008361\n",
      "12/11/21 - 07:57.53PM, iter: 013, cost: 0.007651\n",
      "12/11/21 - 07:57.54PM, iter: 014, cost: 0.007125\n",
      "12/11/21 - 07:57.55PM, iter: 015, cost: 0.006727\n",
      "12/11/21 - 07:57.57PM, iter: 016, cost: 0.006420\n",
      "12/11/21 - 07:57.58PM, iter: 017, cost: 0.006180\n",
      "12/11/21 - 07:57.59PM, iter: 018, cost: 0.005989\n",
      "12/11/21 - 07:58.01PM, iter: 019, cost: 0.005835\n",
      "12/11/21 - 07:58.02PM, iter: 020, cost: 0.005709\n",
      "12/11/21 - 07:58.03PM, iter: 021, cost: 0.005604\n",
      "12/11/21 - 07:58.04PM, iter: 022, cost: 0.005515\n",
      "12/11/21 - 07:58.06PM, iter: 023, cost: 0.005440\n",
      "12/11/21 - 07:58.07PM, iter: 024, cost: 0.005375\n",
      "12/11/21 - 07:58.08PM, iter: 025, cost: 0.005318\n",
      "12/11/21 - 07:58.10PM, iter: 026, cost: 0.005268\n",
      "12/11/21 - 07:58.11PM, iter: 027, cost: 0.005224\n",
      "12/11/21 - 07:58.13PM, iter: 028, cost: 0.005185\n",
      "12/11/21 - 07:58.14PM, iter: 029, cost: 0.005150\n",
      "12/11/21 - 07:58.16PM, iter: 030, cost: 0.005118\n",
      "12/11/21 - 07:58.17PM, iter: 031, cost: 0.005089\n",
      "12/11/21 - 07:58.18PM, iter: 032, cost: 0.005062\n",
      "12/11/21 - 07:58.20PM, iter: 033, cost: 0.005038\n",
      "12/11/21 - 07:58.21PM, iter: 034, cost: 0.005015\n",
      "12/11/21 - 07:58.22PM, iter: 035, cost: 0.004995\n",
      "12/11/21 - 07:58.23PM, iter: 036, cost: 0.004976\n",
      "12/11/21 - 07:58.25PM, iter: 037, cost: 0.004958\n",
      "12/11/21 - 07:58.26PM, iter: 038, cost: 0.004941\n",
      "12/11/21 - 07:58.27PM, iter: 039, cost: 0.004925\n",
      "12/11/21 - 07:58.29PM, iter: 040, cost: 0.004911\n",
      "12/11/21 - 07:58.30PM, iter: 041, cost: 0.004897\n",
      "12/11/21 - 07:58.31PM, iter: 042, cost: 0.004884\n",
      "12/11/21 - 07:58.32PM, iter: 043, cost: 0.004872\n",
      "12/11/21 - 07:58.34PM, iter: 044, cost: 0.004860\n",
      "12/11/21 - 07:58.35PM, iter: 045, cost: 0.004850\n",
      "12/11/21 - 07:58.36PM, iter: 046, cost: 0.004839\n",
      "12/11/21 - 07:58.37PM, iter: 047, cost: 0.004829\n",
      "12/11/21 - 07:58.39PM, iter: 048, cost: 0.004820\n",
      "12/11/21 - 07:58.40PM, iter: 049, cost: 0.004811\n",
      "12/11/21 - 07:58.41PM, iter: 050, cost: 0.004802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(f\"{GBUILD_DIR}/glove -save-file {SAVE_FILE} -threads {THREADS} -input-file {COOCCURRENCE_SHUF_FILE} -eta {ETA} -x-max {X_MAX} -iter {MAX_ITER} -vector-size {VECTOR_SIZE} -vocab-file {VOCAB_FILE} -verbose {VERBOSE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = pd.read_csv(SAVE_FILE + '.txt', sep=' ', index_col=0, header=None).apply(lambda x: np.array(x),axis=1).to_dict()\n",
    "embedding_keys = [x for x in mapping]\n",
    "embedding_mat = np.array([x for x in mapping.values()])\n",
    "tokenized = [[embedding_keys.index(x) for x in seq] for seq in preprocessed_seq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVE_FILE + '.pkl', 'wb') as out:\n",
    "    pickle.dump([data.gene.tolist(), tokenized, embedding_keys, embedding_mat], out)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'../processed_data/utrs_glove_embeddings{Y_ID}.pkl', 'wb') as out:\n",
    "    pickle.dump({'data': [np.mean([embedding_mat[i, :] for i in t],axis=0) for t in tokenized], 'gene': data.gene.tolist()} ,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../processed_data/utrs_embeddings_6_10_50'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAVE_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SAVE_FILE + '.pkl', 'rb') as inp:\n",
    "    genes, tokenized, embedding_keys, embedding_mat = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = tokenized[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaslem/code/intbioproj/.venv/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/vaslem/code/intbioproj/.venv/lib/python3.8/site-packages/numpy/core/_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# split the sequences into 3 parts and add those embeddings\n",
    "import pickle\n",
    "with open(f'../processed_data/utrs_glove_embeddings{Y_ID}_extended.pkl', 'wb') as out:\n",
    "    pickle.dump({'data': [np.hstack([\n",
    "        np.mean([embedding_mat[i, :] for i in t],axis=0) for t in chunks(tok, 3)]) for tok in tokenized], \n",
    "                'gene': data.gene.tolist()} ,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intbioproj",
   "language": "python",
   "name": "intbioproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
