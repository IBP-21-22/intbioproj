{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../processed_data/utrs_embeddings_5_10_50.pkl', 'rb') as inp:\n",
    "    [genes, tokenized, embedding_keys, embedding_mat] = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "fDf = pd.read_csv('../processed_data/fluorescence_dataset.csv')\n",
    "fDf.dropna(subset=['intensity'],inplace=True)\n",
    "common_geneset = set(genes) & set(fDf['gene'])\n",
    "seq_geneset_mask = [g in common_geneset for g in genes]\n",
    "seq_geneset = [g for g in genes if g in common_geneset]\n",
    "tokenized = [s for s,g in zip(tokenized, seq_geneset_mask) if g]\n",
    "fDf = fDf.set_index('gene').loc[seq_geneset, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 106 rows that have negative intensity\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3646, 3646)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = fDf['intensity'] > 0 # negative intensities make no sense\n",
    "print(f\"Removing {sum(~mask)} rows that have negative intensity\")\n",
    "fDf = fDf[mask]\n",
    "tokenized = [x for m,x in zip(mask, tokenized) if m]\n",
    "len(fDf), len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing 6 rows that have outliers log Intensity values <= -20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(3640, 3640)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fDf['logInt'] = np.log(fDf['intensity'])\n",
    "maskLog = fDf['logInt']>-20\n",
    "print(f\"Removing {sum(~maskLog)} rows that have outliers log Intensity values <= -20\")\n",
    "fDf = fDf[fDf['logInt']>-20]\n",
    "tokenized = [x for m,x in zip(maskLog, tokenized) if m]\n",
    "y = fDf['logInt'].copy()\n",
    "len(fDf), len(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dig = np.digitize(y, np.quantile(y, np.arange(0,1,0.1)))\n",
    "y_bin = y_dig >= 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "CLIP = 0.25\n",
    "LOG_INTERVAL = 200\n",
    "SEQ_LEN = 35\n",
    "LR = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, y_train, x_test, y_test = train_test_split(tokenized, y, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.ragged import constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vaslem/code/intbioproj/.venv/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_42/multiply_28/RaggedTile_1/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/model_42/multiply_28/RaggedTile_1/Reshape_2:0\", shape=(None, 1), dtype=float16), dense_shape=Tensor(\"gradient_tape/model_42/multiply_28/RaggedTile_1/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/home/vaslem/code/intbioproj/.venv/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_42/softmax_28/RaggedSoftmax/RaggedTile/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/model_42/softmax_28/RaggedSoftmax/RaggedTile/Reshape_2:0\", shape=(None, 1), dtype=float16), dense_shape=Tensor(\"gradient_tape/model_42/softmax_28/RaggedSoftmax/RaggedTile/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/home/vaslem/code/intbioproj/.venv/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/model_42/multiply_28/RaggedTile/Reshape_3:0\", shape=(None,), dtype=int64), values=Tensor(\"gradient_tape/model_42/multiply_28/RaggedTile/Reshape_2:0\", shape=(None, 128), dtype=float16), dense_shape=Tensor(\"gradient_tape/model_42/multiply_28/RaggedTile/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/home/vaslem/code/intbioproj/.venv/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:448: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"Adam/gradients/concat_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/model_42/embedding_30/Cast_1/Cast:0\", shape=(None, 50), dtype=float32), dense_shape=Tensor(\"gradient_tape/model_42/bidirectional_2/forward_lstm_28/RaggedToTensor/Shape:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 218s 2s/step - loss: 0.6713\n",
      "Epoch 2/30\n",
      "  6/114 [>.............................] - ETA: 4:22 - loss: 0.6535"
     ]
    }
   ],
   "source": [
    "# Tensorflow 1.9; Keras 2.2.0 (latest versions)\n",
    "# should be backwards compatible upto Keras 2.0.9 and tf 1.5\n",
    "from keras.models import Model\n",
    "from keras.layers import *\n",
    "import numpy as np\n",
    "from tensorflow.keras import mixed_precision\n",
    "policy = mixed_precision.Policy('mixed_float16')\n",
    "mixed_precision.set_global_policy(policy)\n",
    "def create_models():\n",
    "    #Get a sequence of indexes of words as input:\n",
    "    # Keras supports dynamic input lengths if you provide (None,) as the \n",
    "    #  input shape\n",
    "    inp = Input((None,))\n",
    "    #Embed words into vectors of size 10 each:\n",
    "    # Output shape is (None,10)\n",
    "    embs = Embedding(embedding_mat.shape[0],\n",
    "                            embedding_mat.shape[1],\n",
    "                            weights=[embedding_mat],\n",
    "                            trainable=False)(inp)\n",
    "    # Run LSTM on these vectors and return output on each timestep\n",
    "    # Output shape is (None,5)\n",
    "    lstm = Bidirectional(LSTM(64, return_sequences=True))(embs)\n",
    "    ##Attention Block\n",
    "    #Transform each timestep into 1 value (attention_value) \n",
    "    # Output shape is (None,1)\n",
    "    attention = TimeDistributed(Dense(1))(lstm)\n",
    "    #By running softmax on axis 1 we force attention_values\n",
    "    # to sum up to 1. We are effectively assigning a \"weight\" to each timestep\n",
    "    # Output shape is still (None,1) but each value changes\n",
    "    attention_vals = Softmax(axis=1)(attention)\n",
    "    # Multiply the encoded timestep by the respective weight\n",
    "    # I.e. we are scaling each timestep based on its weight\n",
    "    # Output shape is (None,5): (None,5)*(None,1)=(None,5)\n",
    "    scaled_vecs = Multiply()([lstm,attention_vals])\n",
    "    # Sum up all scaled timesteps into 1 vector \n",
    "    # i.e. obtain a weighted sum of timesteps\n",
    "    # Output shape is (5,) : Observe the time dimension got collapsed\n",
    "    context_vector = Lambda(lambda x: tf.keras.backend.sum(x,axis=1))(scaled_vecs)\n",
    "    ##Attention Block over\n",
    "    # Get the output out\n",
    "    x = Dense(1)(context_vector)\n",
    "    out = Activation('sigmoid', dtype='float32', name='predictions')(x)\n",
    "    model = Model(inp, out)\n",
    "    model_with_attention_output = Model(inp, [out, attention_vals])\n",
    "    model.compile(optimizer='adam',loss='binary_crossentropy')\n",
    "    return model, model_with_attention_output\n",
    "\n",
    "model,model_with_attention_output = create_models()\n",
    "\n",
    "\n",
    "model.fit(constant([np.array(t) for t in tokenized],dtype='int16'),np.array(y_bin),batch_size=32, epochs=30)\n",
    "print ('Attention Over each word: ',model_with_attention_output.predict(np.array([[1,2,3]]),batch_size=1)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intbioproj",
   "language": "python",
   "name": "intbioproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
